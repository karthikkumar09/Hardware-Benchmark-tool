# Automated Hardware Benchmarking & Performance Comparison Tool

A comprehensive Python-based automation framework for evaluating and comparing hardware performance across heterogeneous platforms. This tool reduces manual benchmarking effort by 70% while achieving high repeatability with sub-5% variance.

![Hardware Benchmarking](https://img.shields.io/badge/Python-3.8%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success)

## ğŸ¯ Project Overview

This project solves the challenge of objective hardware performance comparison for capacity planning decisions. Manual benchmarking is time-consuming, inconsistent, and difficult to compare across different systems. This automated framework standardizes the process and provides actionable insights.

### Key Features

- âœ… **Automated Benchmarking**: CPU, Memory, Disk I/O, and Network performance testing
- âœ… **High Repeatability**: Achieves <5% variance across multiple runs (tested at 1.6% average)
- âœ… **Normalized Scoring**: Converts raw metrics to comparable 0-100 scores
- âœ… **Multi-System Comparison**: Side-by-side comparison with visual charts
- âœ… **Capacity Planning**: Workload-specific hardware recommendations
- âœ… **Comprehensive Reporting**: JSON, CSV, HTML, and TXT formats

## ğŸ“Š Sample Results

### Individual Benchmark Report
![Benchmark Report](docs/benchmark_report_sample.png)

### Multi-System Comparison
![Comparison Report](docs/comparison_report_sample.png)

**Key Metrics Achieved:**
- CPU Variance: 0.13% - 3.54%
- Memory Variance: 0.47% - 1.60%
- Overall Average: 1.6% (Well below 5% target)

## ğŸš€ Quick Start

### Prerequisites

**System Requirements:**
- Python 3.8 or higher
- Linux (Ubuntu/Debian recommended) or WSL2 on Windows
- Root/sudo access for some benchmarks

**Install Benchmark Tools:**
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install sysbench fio iperf3

# macOS
brew install sysbench fio iperf3
```

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/hardware-benchmark-tool.git
cd hardware-benchmark-tool
```

2. **Create virtual environment:**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install Python dependencies:**
```bash
pip install -r requirements.txt
```

### Basic Usage

**Run a quick test:**
```bash
python main.py --quick
```

**Run full benchmark suite:**
```bash
python main.py
```

**Run specific benchmarks:**
```bash
python main.py --cpu-only
```

## ğŸ“ Project Structure
```
hardware-benchmark-tool/
â”œâ”€â”€ main.py                          # Main benchmark controller
â”œâ”€â”€ comparison_tool.py               # Multi-system comparison
â”œâ”€â”€ capacity_planner.py              # Capacity planning module
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ benchmark_config.yaml       # Configuration settings
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ cpu_benchmark.py            # CPU benchmarking
â”‚   â”œâ”€â”€ memory_benchmark.py         # Memory benchmarking
â”‚   â”œâ”€â”€ disk_benchmark.py           # Disk I/O benchmarking
â”‚   â””â”€â”€ network_benchmark.py        # Network benchmarking
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_normalizer.py          # Result normalization
â”‚   â””â”€â”€ report_generator.py         # Report generation
â”œâ”€â”€ results/                         # Benchmark outputs
â””â”€â”€ reports/                         # Comparison reports
```

## ğŸ”§ Configuration

Edit `config/benchmark_config.yaml` to customize:
```yaml
runs: 3  # Number of runs per benchmark

cpu:
  enabled: true
  threads: 4
  duration: 30  # seconds

memory:
  enabled: true
  block_size: 1M
  total_size: 10G

disk:
  enabled: true
  size: 1G
  runtime: 30
```

## ğŸ“ˆ Advanced Features

### Multi-System Comparison

Compare multiple hardware configurations:
```bash
python comparison_tool.py \
  results/system1 \
  results/system2 \
  results/system3 \
  -n "Laptop" "Server-A" "Server-B" \
  -o reports
```

**Output:**
- Comparison charts (4 visualizations)
- Detailed comparison table (CSV)
- HTML report with performance winners
- Variance analysis across systems

### Capacity Planning

Get hardware recommendations based on workload:
```bash
python capacity_planner.py \
  --workload database \
  --systems results/system1 results/system2 \
  --names "Config-A" "Config-B" \
  --costs 1000 1500 \
  --output capacity_plan.txt
```

**Workload Types:**
- `web_server` - Prioritizes CPU and memory
- `database` - Prioritizes memory and disk
- `file_server` - Prioritizes disk and network
- `compute_intensive` - Prioritizes CPU
- `general_purpose` - Balanced across all components

## ğŸ“Š Understanding Results

### Normalized Scores (0-100)
- **90-100**: Excellent performance
- **75-89**: Very good performance
- **60-74**: Good performance
- **40-59**: Fair performance
- **<40**: Needs improvement

### Variance Interpretation
- **<5%**: Excellent repeatability âœ…
- **5-10%**: Good repeatability
- **>10%**: Consider increasing test runs

## ğŸ¯ Use Cases

### 1. Hardware Selection
Compare multiple hardware options before procurement:
```bash
# Benchmark each candidate system
python main.py

# Compare all systems
python comparison_tool.py results/* -o procurement_analysis
```

### 2. Performance Validation
Verify new hardware meets requirements:
```bash
python capacity_planner.py --workload database --systems results/new_server
```

### 3. Regression Testing
Detect performance degradation after updates:
```bash
# Before update
python main.py && mv results/* results/before_update/

# After update
python main.py && mv results/* results/after_update/

# Compare
python comparison_tool.py results/before_update results/after_update
```

## ğŸ”¬ Technical Details

### Benchmarking Tools
- **sysbench** - Industry-standard CPU/Memory benchmarking
- **fio** - Flexible I/O tester (used by major cloud providers)
- **iperf3** - Network performance measurement standard

### Normalization Algorithm
Uses min-max scaling to normalize raw metrics:
```
normalized_score = ((value - min) / (max - min)) Ã— 100
```

Baseline values are configurable in `utils/data_normalizer.py`

### Statistical Analysis
Each benchmark runs multiple times (default: 3) to calculate:
- Mean
- Median
- Standard deviation
- Variance percentage
- Min/Max values

## ğŸ“ Output Formats

### 1. Raw Results (JSON)
```json
{
  "cpu": {
    "statistics": {
      "events_per_second": {
        "mean": 3287.17,
        "variance_percent": 3.54
      }
    }
  }
}
```

### 2. Normalized Results (JSON)
```json
{
  "overall_score": 32.13,
  "cpu": {
    "score": 32.19,
    "variance": 3.54
  }
}
```

### 3. Text Report
Human-readable summary with system info and ratings

### 4. HTML Report
Interactive visual report with charts and metrics

### 5. CSV Export
Spreadsheet-ready comparison tables

## ğŸ† Performance Benchmarks

**Tested on:** AMD Ryzen / Intel Core systems with WSL2

| Component | Score | Variance | Status |
|-----------|-------|----------|--------|
| CPU | 32-38/100 | 0.13-3.54% | âœ… |
| Memory | 51-57/100 | 0.47-1.60% | âœ… |
| Disk I/O | 17-19/100 | <5% | âœ… |
| **Overall** | **32-35/100** | **1.6% avg** | âœ… |

## ğŸ› ï¸ Troubleshooting

### High Variance (>10%)
- Close background applications
- Disable CPU frequency scaling
- Increase test duration in config
- Run during low system activity

### Benchmark Tool Not Found
```bash
# Install missing tools
sudo apt-get install sysbench fio iperf3
```

### Permission Denied (Disk Tests)
```bash
# Use /tmp directory or run with sudo
sudo python main.py
```

## ğŸ“š Documentation

- [Installation Guide](docs/installation.md)
- [Configuration Reference](docs/configuration.md)
- [API Documentation](docs/api.md)
- [Contributing Guidelines](CONTRIBUTING.md)

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**GORIGA KARTHIK KUMAR**
- GitHub: [@karthikkumar09](https://github.com/karthikkumar09)
- LinkedIn: [karthikkumar](https://www.linkedin.com/in/karthik-kumar-781bb32b4/)
- Email: gkklucky7@gmail.com

## ğŸ™ Acknowledgments

- sysbench team for CPU/Memory benchmarking tools
- fio developers for flexible I/O testing
- iperf3 project for network performance testing
- Python community for excellent libraries (pandas, matplotlib)

## ğŸ“Š Project Status

âœ… **Production Ready** - All core features implemented and tested

**Recent Updates:**
- âœ… Multi-system comparison tool
- âœ… Capacity planning module
- âœ… Sub-5% variance achievement
- âœ… Comprehensive reporting suite

## ğŸ”® Future Enhancements

- [ ] GPU benchmarking support
- [ ] Database storage for historical results
- [ ] Web dashboard interface
- [ ] Docker containerization
- [ ] Automated email reporting
- [ ] Integration with monitoring systems (Prometheus/Grafana)

---

**â­ If you find this project useful, please star it on GitHub!**
